---
title: "Survival regression with accelerated failure time model in XGBoost"
collection: publications
permalink: /publication/2020-06-15-arxiv
excerpt: 'Survival month for non-small lung cancer patients depend upon which stage of lung cancer is present. Our aim is to identify smoking specific gene expression biomarkers in prognosis of lung cancer patients. In this paper, we introduce the network elastic net, a generalization of network lasso that allows for simultaneous clustering and regression on graphs. In network elastic net, we consider similar patients based on smoking cigarettes per year to form the network. We then further find the suitable cluster among patients based on coefficients of genes having different survival month structures and showed the efficacy of the clusters using stage enrichment. This can be used to identify the stage of cancer using gene expression and smoking behavior of patients without doing any tests.'
date: 2020-06-15
venue: 'Arxiv'
paperurl: 'https://arxiv.org/pdf/2006.04920.pdf'
citation: 'Barnwal, Avinash et al. “Survival regression with accelerated failure time model in XGBoost.” ArXiv abs/2006.04920 (2020): n. pag.'
---
Survival regression is used to estimate the relation between time-to-event and feature variables, and is important in application domains such as medicine, marketing, risk management and sales management. Nonlinear tree based machine learning algorithms as implemented in libraries such as XGBoost, scikit-learn, LightGBM, and CatBoost are often more accurate in practice than linear models. However, existing implementations of tree-based models have offered limited support for survival regression. In this work, we propose and implement loss functions for learning accelerated failure time (AFT) models in XGBoost, to increase the support for survival modeling for different kinds of label censoring. The AFT model assumes effects that directly accelerate or decelerate the survival time for different kinds of censored data sets. We demonstrate with real and simulated experiments the effectiveness of AFT in XGBoost with respect to a number of baselines, in two respects: generalization performance and training speed. Furthermore, we take advantage of the support for NVIDIA GPUs in XGBoost to achieve substantial speedup over multi-coreCPUs. To our knowledge, our work is the first implementation of AFT that utilizes the processing power of NVIDIA GPUs.
[Download paper here](https://arxiv.org/pdf/2006.04920.pdf)
